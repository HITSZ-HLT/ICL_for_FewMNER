{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class InputExample(object):\n",
    "    def __init__(self, text_a, text_b=None, label_aspect=None, image_id=None):\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label_aspect = label_aspect\n",
    "        self.image_id = image_id\n",
    "\n",
    "def read_mner(text_path):\n",
    "    load_file = text_path\n",
    "    examples = []\n",
    "    count = 0\n",
    "    with open(load_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        raw_words, raw_targets = [], []\n",
    "        raw_word, raw_target = [], []\n",
    "        imgs = []\n",
    "        for line in lines:\n",
    "            if line.startswith(\"IMGID:\"):\n",
    "                img_id = line.strip().split('IMGID:')[1] + '.jpg'\n",
    "                imgs.append(img_id)\n",
    "                continue\n",
    "            if line != \"\\n\":\n",
    "                raw_word.append(line.split('\\t')[0])\n",
    "                label = line.split('\\t')[1][:-1]\n",
    "                if 'OTHER' in label:\n",
    "                    label = label[:2] + 'MISC'\n",
    "                raw_target.append(label)\n",
    "            else:\n",
    "                assert len(raw_word) == len(raw_target)\n",
    "                raw_words.append(raw_word)\n",
    "                raw_targets.append(raw_target)\n",
    "                # guid = \"%s-%s\" % (set_type, count)\n",
    "                text_a = ' '.join(raw_word)\n",
    "                tags = raw_target\n",
    "                image_path_single = str(img_id)\n",
    "                count += 1\n",
    "                examples.append(\n",
    "                    InputExample(text_a=text_a, text_b=None, label_aspect=tags, image_id=image_path_single)\n",
    "                )\n",
    "                raw_word, raw_target = [], []\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts2bio(ts_tag_sequence):\n",
    "    \n",
    "    new_ts_sequence = []\n",
    "    n_tags = len(ts_tag_sequence)\n",
    "    for i in range(n_tags):\n",
    "        ts_tag = ts_tag_sequence[i]\n",
    "        if ts_tag == 'O' or ts_tag == 'EQ':\n",
    "            new_ts_sequence.append('O')\n",
    "        else:\n",
    "            new_ts_sequence.append(ts_tag)\n",
    "    return new_ts_sequence\n",
    "\n",
    "def bio2bioes_ts(ts_tag_sequence):\n",
    "    n_tags = len(ts_tag_sequence)\n",
    "    new_ts_sequence = []\n",
    "    for i in range(n_tags):\n",
    "        cur_ts_tag = ts_tag_sequence[i]\n",
    "        if cur_ts_tag == 'O' or cur_ts_tag == 'EQ':\n",
    "            # when meet the EQ label, regard it as O label\n",
    "            new_ts_sequence.append('O')\n",
    "        else:\n",
    "            cur_pos, cur_sentiment = cur_ts_tag.split('-')\n",
    "            if cur_pos == 'B':\n",
    "                if (i == n_tags - 1) or (ts_tag_sequence[i+1].split('-')[0] != 'I'):\n",
    "                    new_ts_sequence.append('S-%s' % cur_sentiment)\n",
    "                else:\n",
    "                    new_ts_sequence.append('B-%s' % cur_sentiment)\n",
    "            elif cur_pos == 'I':\n",
    "                # if (i == n_tags - 1) or (ts_tag_sequence[i+1].split('-')[0] != 'I'):\n",
    "                # 少考虑这种情况：[O, O, I, O, B, I, I, O, O]\n",
    "                # 第一个I，如果按照原来的规则，会被标记为E\n",
    "                if (i == n_tags - 1) or (ts_tag_sequence[i + 1].split('-')[0] != 'I' and i != 0 and ts_tag_sequence[i - 1].split('-')[0] != 'O'):\n",
    "                    new_ts_sequence.append('E-%s' % cur_sentiment)\n",
    "                else:\n",
    "                    new_ts_sequence.append('I-%s' % cur_sentiment)\n",
    "    return new_ts_sequence\n",
    "\n",
    "def tag2ts(ts_tag_sequence):\n",
    "    \"\"\"\n",
    "    transform ts tag sequence to targeted sentiment\n",
    "    :param ts_tag_sequence: tag sequence for ts task\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    n_tags = len(ts_tag_sequence)\n",
    "    ts_sequence, sentiments = [], []\n",
    "    beg, end = -1, -1\n",
    "    for i in range(n_tags):\n",
    "        ts_tag = ts_tag_sequence[i]\n",
    "        # current position and sentiment\n",
    "        # tag O and tag EQ will not be counted\n",
    "        eles = ts_tag.split('-')\n",
    "        if len(eles) == 2:\n",
    "            pos, sentiment = eles\n",
    "        else:\n",
    "            pos, sentiment = 'O', 'O'\n",
    "        if sentiment != 'O':\n",
    "            # current word is a subjective word\n",
    "            sentiments.append(sentiment)\n",
    "        if pos == 'S':\n",
    "            # singleton\n",
    "            ts_sequence.append((i, i, sentiment))\n",
    "            sentiments = []\n",
    "        elif pos == 'B':\n",
    "            beg = i\n",
    "            if len(sentiments) > 1:\n",
    "                # remove the effect of the noisy I-{POS,NEG,NEU}\n",
    "                sentiments = [sentiments[-1]]\n",
    "        elif pos == 'E':\n",
    "            end = i\n",
    "            # schema1: only the consistent sentiment tags are accepted\n",
    "            # that is, all of the sentiment tags are the same\n",
    "            if end > beg > -1 and len(set(sentiments)) == 1:\n",
    "                ts_sequence.append((beg, end, sentiment))\n",
    "                sentiments = []\n",
    "                beg, end = -1, -1\n",
    "    return ts_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_count 2218\n",
      "loc_count 2088\n",
      "org_count 925\n",
      "misc_count 931\n",
      "per_count 552\n",
      "loc_count 522\n",
      "org_count 247\n",
      "misc_count 219\n",
      "per_count 1825\n",
      "loc_count 1728\n",
      "org_count 839\n",
      "misc_count 724\n",
      "4000 1000 3257\n"
     ]
    }
   ],
   "source": [
    "# multi entities\n",
    "import pandas as pd\n",
    "def data_to_pandas(examples):\n",
    "    pd_test = pd.DataFrame(columns=[\"text\", \"entity_category\", \"image_id\", \"person\", \"location\", \"organization\", \"miscellaneous\"])\n",
    "    per_count, loc_count, org_count, misc_count = 0, 0, 0, 0\n",
    "    for i in range(len(examples)):\n",
    "        text = examples[i].text_a\n",
    "        label_aspect = examples[i].label_aspect\n",
    "        image_id = examples[i].image_id\n",
    "        tags = bio2bioes_ts(ts2bio(label_aspect))\n",
    "        ts_sequence = tag2ts(ts_tag_sequence=tags)\n",
    "        text_list = text.split(\" \")\n",
    "        entity_list,category_list = [], []\n",
    "        person_list, location_list, org_list, misc_list = [], [], [], []\n",
    "        for ner in ts_sequence:\n",
    "            entity = \" \".join(text_list[ner[0]:ner[1]+1])\n",
    "            category = ner[-1]\n",
    "            if category == \"MISC\":\n",
    "                category = \"miscellaneous\"\n",
    "                misc_list.append(entity)\n",
    "                misc_count += 1\n",
    "            elif category == \"LOC\":\n",
    "                category = \"location\"\n",
    "                location_list.append(entity)\n",
    "                loc_count += 1\n",
    "            elif category == \"ORG\":\n",
    "                category = \"organization\"\n",
    "                org_list.append(entity)\n",
    "                org_count += 1\n",
    "            elif category == \"PER\":\n",
    "                category = \"person\"\n",
    "                person_list.append(entity)\n",
    "                per_count += 1\n",
    "            else:\n",
    "                raise ValueError(\"error category\")\n",
    "            entity_list.append(entity)\n",
    "            category_list.append(category)\n",
    "        entity_category = []\n",
    "        for j in range(len(entity_list)):\n",
    "            entity_category.append((entity_list[j], category_list[j]))\n",
    "        pd_test.loc[i, \"text\"] = text\n",
    "        pd_test.loc[i, \"entity_category\"] = entity_category\n",
    "        pd_test.loc[i, \"image_id\"] = image_id\n",
    "        \n",
    "        # \"person\", \"location\", \"organization\", \"miscellaneous\"\n",
    "        pd_test.loc[i, \"person\"] = person_list\n",
    "        pd_test.loc[i, \"location\"] = location_list\n",
    "        pd_test.loc[i, \"organization\"] = org_list\n",
    "        pd_test.loc[i, \"miscellaneous\"] = misc_list\n",
    "    print(\"per_count\", per_count)\n",
    "    print(\"loc_count\", loc_count)\n",
    "    print(\"org_count\", org_count)\n",
    "    print(\"misc_count\", misc_count)\n",
    "    return pd_test\n",
    "\n",
    "train_path = \"../dataset/mner/twitter2015/train.txt\"\n",
    "dev_path = \"../dataset/mner/twitter2015/dev.txt\"\n",
    "test_path = \"../dataset/mner/twitter2015/test.txt\"\n",
    "\n",
    "train_examples = read_mner(train_path)\n",
    "dev_examples = read_mner(dev_path)\n",
    "test_examples = read_mner(test_path)\n",
    "\n",
    "pd_train = data_to_pandas(examples=train_examples)\n",
    "pd_dev = data_to_pandas(examples=dev_examples)\n",
    "pd_test = data_to_pandas(examples=test_examples)\n",
    "\n",
    "print(len(pd_train), len(pd_dev), len(pd_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pd_train.loc[0, \"person\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save processed files\n",
    "pd_train.to_csv(\"twitter2015_process_train.csv\", index=False, sep='\\t')\n",
    "pd_dev.to_csv(\"twitter2015_process_dev.csv\", index=False, sep='\\t')\n",
    "pd_test.to_csv(\"twitter2015_process_test.csv\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_test = pd.read_csv(\"twitter2015_process_train.csv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>entity_category</th>\n",
       "      <th>image_id</th>\n",
       "      <th>person</th>\n",
       "      <th>location</th>\n",
       "      <th>organization</th>\n",
       "      <th>miscellaneous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @JayKenMinaj _ : Me outside of where George...</td>\n",
       "      <td>[('George Zimmerman', 'person')]</td>\n",
       "      <td>1015799.jpg</td>\n",
       "      <td>['George Zimmerman']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Swan upping : first stop Hermitage Warf , Towe...</td>\n",
       "      <td>[('Hermitage Warf', 'location'), ('Tower Bridg...</td>\n",
       "      <td>1109405.jpg</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Hermitage Warf', 'Tower Bridge', 'Tower']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Olympic']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @redbullESPORTS : Smash Shiba is stoked for...</td>\n",
       "      <td>[('Smash Shiba', 'miscellaneous'), ('Melee Gra...</td>\n",
       "      <td>563049.jpg</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Smash Shiba', 'Melee Grand Finals']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @washingtonpost : Two maps that show the sh...</td>\n",
       "      <td>[('Baltimore', 'location')]</td>\n",
       "      <td>50447.jpg</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Baltimore']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rep . Howard Coble mingling ahead of press con...</td>\n",
       "      <td>[('Howard Coble', 'person')]</td>\n",
       "      <td>418340.jpg</td>\n",
       "      <td>['Howard Coble']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Psychologists explain why Katie Hopkins is jus...</td>\n",
       "      <td>[('Katie Hopkins', 'person'), ('Katie #Hopkins...</td>\n",
       "      <td>50168.jpg</td>\n",
       "      <td>['Katie Hopkins', 'Katie #Hopkins']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Do you even meditate tho bro ? I start class a...</td>\n",
       "      <td>[('LA', 'location')]</td>\n",
       "      <td>684051.jpg</td>\n",
       "      <td>[]</td>\n",
       "      <td>['LA']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @cnni : Pope Francis calls for an end to th...</td>\n",
       "      <td>[('Pope Francis', 'person')]</td>\n",
       "      <td>1351124.jpg</td>\n",
       "      <td>['Pope Francis']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @ShervinSinatra : Amsterdam Savage AF . htt...</td>\n",
       "      <td>[('Amsterdam Savage AF', 'organization')]</td>\n",
       "      <td>94770.jpg</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Amsterdam Savage AF']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @jordancornette : This is how Alex Rodrigue...</td>\n",
       "      <td>[('Alex Rodriguez', 'person'), ('Notre Dame', ...</td>\n",
       "      <td>64931.jpg</td>\n",
       "      <td>['Alex Rodriguez', 'ARod']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Notre Dame']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RT @AuthorGiaLee : I ' ve titled this one \" In...</td>\n",
       "      <td>[('Columbia', 'location'), ('SC', 'location')]</td>\n",
       "      <td>63944.jpg</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Columbia', 'SC']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Jillian enjoys a birthday bath--and a blue sno...</td>\n",
       "      <td>[('Jillian', 'person'), ('Sandi Wong', 'person')]</td>\n",
       "      <td>1285253.jpg</td>\n",
       "      <td>['Jillian', 'Sandi Wong']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RT @noisywoman : @davrosz @SamAntixMusic This ...</td>\n",
       "      <td>[('Peter Dutton', 'person')]</td>\n",
       "      <td>1330809.jpg</td>\n",
       "      <td>['Peter Dutton']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RT @victoiregarnier : Alex had a great 8th Bir...</td>\n",
       "      <td>[('Alex', 'person')]</td>\n",
       "      <td>317066.jpg</td>\n",
       "      <td>['Alex']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RT @TMZ : \" Walking Dead \" star , Seth Gilliam...</td>\n",
       "      <td>[('Walking Dead', 'miscellaneous'), ('Seth Gil...</td>\n",
       "      <td>589703.jpg</td>\n",
       "      <td>['Seth Gilliam']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Walking Dead', 'DUI']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AK Park , Wabash Ave http://t.co/BTrsUnTlos</td>\n",
       "      <td>[('AK Park', 'location'), ('Wabash Ave', 'loca...</td>\n",
       "      <td>72512.jpg</td>\n",
       "      <td>[]</td>\n",
       "      <td>['AK Park', 'Wabash Ave']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Looking for a great moms group in Peoria ? Joi...</td>\n",
       "      <td>[('Peoria', 'location'), ('#peoria', 'location')]</td>\n",
       "      <td>1299485.jpg</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Peoria', '#peoria']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RT @LFC : #LFC can confirm that Steven Gerrard...</td>\n",
       "      <td>[('#LFC', 'organization'), ('Steven Gerrard', ...</td>\n",
       "      <td>268501.jpg</td>\n",
       "      <td>['Steven Gerrard']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['#LFC']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RT @JimBeam : Egypt has pyramids , we have rac...</td>\n",
       "      <td>[('Egypt', 'location'), ('Jim Beam American St...</td>\n",
       "      <td>151567.jpg</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Egypt', 'Jim Beam American Stillhouse']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RT @NBABeLike : Paul Pierce be like http://t.c...</td>\n",
       "      <td>[('Paul Pierce', 'person')]</td>\n",
       "      <td>12609.jpg</td>\n",
       "      <td>['Paul Pierce']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   RT @JayKenMinaj _ : Me outside of where George...   \n",
       "1   Swan upping : first stop Hermitage Warf , Towe...   \n",
       "2   RT @redbullESPORTS : Smash Shiba is stoked for...   \n",
       "3   RT @washingtonpost : Two maps that show the sh...   \n",
       "4   Rep . Howard Coble mingling ahead of press con...   \n",
       "5   Psychologists explain why Katie Hopkins is jus...   \n",
       "6   Do you even meditate tho bro ? I start class a...   \n",
       "7   RT @cnni : Pope Francis calls for an end to th...   \n",
       "8   RT @ShervinSinatra : Amsterdam Savage AF . htt...   \n",
       "9   RT @jordancornette : This is how Alex Rodrigue...   \n",
       "10  RT @AuthorGiaLee : I ' ve titled this one \" In...   \n",
       "11  Jillian enjoys a birthday bath--and a blue sno...   \n",
       "12  RT @noisywoman : @davrosz @SamAntixMusic This ...   \n",
       "13  RT @victoiregarnier : Alex had a great 8th Bir...   \n",
       "14  RT @TMZ : \" Walking Dead \" star , Seth Gilliam...   \n",
       "15        AK Park , Wabash Ave http://t.co/BTrsUnTlos   \n",
       "16  Looking for a great moms group in Peoria ? Joi...   \n",
       "17  RT @LFC : #LFC can confirm that Steven Gerrard...   \n",
       "18  RT @JimBeam : Egypt has pyramids , we have rac...   \n",
       "19  RT @NBABeLike : Paul Pierce be like http://t.c...   \n",
       "\n",
       "                                      entity_category     image_id  \\\n",
       "0                    [('George Zimmerman', 'person')]  1015799.jpg   \n",
       "1   [('Hermitage Warf', 'location'), ('Tower Bridg...  1109405.jpg   \n",
       "2   [('Smash Shiba', 'miscellaneous'), ('Melee Gra...   563049.jpg   \n",
       "3                         [('Baltimore', 'location')]    50447.jpg   \n",
       "4                        [('Howard Coble', 'person')]   418340.jpg   \n",
       "5   [('Katie Hopkins', 'person'), ('Katie #Hopkins...    50168.jpg   \n",
       "6                                [('LA', 'location')]   684051.jpg   \n",
       "7                        [('Pope Francis', 'person')]  1351124.jpg   \n",
       "8           [('Amsterdam Savage AF', 'organization')]    94770.jpg   \n",
       "9   [('Alex Rodriguez', 'person'), ('Notre Dame', ...    64931.jpg   \n",
       "10     [('Columbia', 'location'), ('SC', 'location')]    63944.jpg   \n",
       "11  [('Jillian', 'person'), ('Sandi Wong', 'person')]  1285253.jpg   \n",
       "12                       [('Peter Dutton', 'person')]  1330809.jpg   \n",
       "13                               [('Alex', 'person')]   317066.jpg   \n",
       "14  [('Walking Dead', 'miscellaneous'), ('Seth Gil...   589703.jpg   \n",
       "15  [('AK Park', 'location'), ('Wabash Ave', 'loca...    72512.jpg   \n",
       "16  [('Peoria', 'location'), ('#peoria', 'location')]  1299485.jpg   \n",
       "17  [('#LFC', 'organization'), ('Steven Gerrard', ...   268501.jpg   \n",
       "18  [('Egypt', 'location'), ('Jim Beam American St...   151567.jpg   \n",
       "19                        [('Paul Pierce', 'person')]    12609.jpg   \n",
       "\n",
       "                                 person  \\\n",
       "0                  ['George Zimmerman']   \n",
       "1                                    []   \n",
       "2                                    []   \n",
       "3                                    []   \n",
       "4                      ['Howard Coble']   \n",
       "5   ['Katie Hopkins', 'Katie #Hopkins']   \n",
       "6                                    []   \n",
       "7                      ['Pope Francis']   \n",
       "8                                    []   \n",
       "9            ['Alex Rodriguez', 'ARod']   \n",
       "10                                   []   \n",
       "11            ['Jillian', 'Sandi Wong']   \n",
       "12                     ['Peter Dutton']   \n",
       "13                             ['Alex']   \n",
       "14                     ['Seth Gilliam']   \n",
       "15                                   []   \n",
       "16                                   []   \n",
       "17                   ['Steven Gerrard']   \n",
       "18                                   []   \n",
       "19                      ['Paul Pierce']   \n",
       "\n",
       "                                       location             organization  \\\n",
       "0                                            []                       []   \n",
       "1   ['Hermitage Warf', 'Tower Bridge', 'Tower']                       []   \n",
       "2                                            []                       []   \n",
       "3                                 ['Baltimore']                       []   \n",
       "4                                            []                       []   \n",
       "5                                            []                       []   \n",
       "6                                        ['LA']                       []   \n",
       "7                                            []                       []   \n",
       "8                                            []  ['Amsterdam Savage AF']   \n",
       "9                                            []           ['Notre Dame']   \n",
       "10                           ['Columbia', 'SC']                       []   \n",
       "11                                           []                       []   \n",
       "12                                           []                       []   \n",
       "13                                           []                       []   \n",
       "14                                           []                       []   \n",
       "15                    ['AK Park', 'Wabash Ave']                       []   \n",
       "16                        ['Peoria', '#peoria']                       []   \n",
       "17                                           []                 ['#LFC']   \n",
       "18    ['Egypt', 'Jim Beam American Stillhouse']                       []   \n",
       "19                                           []                       []   \n",
       "\n",
       "                            miscellaneous  \n",
       "0                                      []  \n",
       "1                             ['Olympic']  \n",
       "2   ['Smash Shiba', 'Melee Grand Finals']  \n",
       "3                                      []  \n",
       "4                                      []  \n",
       "5                                      []  \n",
       "6                                      []  \n",
       "7                                      []  \n",
       "8                                      []  \n",
       "9                                      []  \n",
       "10                                     []  \n",
       "11                                     []  \n",
       "12                                     []  \n",
       "13                                     []  \n",
       "14                ['Walking Dead', 'DUI']  \n",
       "15                                     []  \n",
       "16                                     []  \n",
       "17                                     []  \n",
       "18                                     []  \n",
       "19                                     []  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_test.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ricky_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
